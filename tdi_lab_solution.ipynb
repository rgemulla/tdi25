{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f892745",
   "metadata": {},
   "source": [
    "\n",
    "# TDI 2025 Lab I: Big Data Processing\n",
    "\n",
    "## Preliminaries (Imports, Starting Spark, & Defining MapReduce Framework)\n",
    "\n",
    "Before getting started, please upload the files `shakespeare.txt` and\n",
    "`social_network_edges.tsv` to the `content` directory in Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63adab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18fa67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkConf configuration objects store the configuration needed to access the\n",
    "# cluster\n",
    "conf = SparkConf().setAppName(\"myapp\").setMaster(\"local[*]\")\n",
    "\n",
    "# SparkContext objects are used to access the cluster\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# This disables detailed log messages. Try also INFO (more detailed logging)\n",
    "# or ERROR (less logging).\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7d72bd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Implementation of MapReduce on top of Apache Spark\n",
    "\n",
    "This implementation is provided so that it is sufficient to install Apache Spark\n",
    "to use (Hadoop) MapReduce.\n",
    "You can ignore this for now but may want to check it out later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5324ac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class MapReduceProgram:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # -- User-defined methods (implemented in subclasses) -------------------------------------------------\n",
    "\n",
    "    # User-defined Map function. Default outputs nothing.\n",
    "    def map(self, key, value):\n",
    "        pass\n",
    "\n",
    "    # User-defined Combine function. Default does nothing.\n",
    "    def combine(self, key, values):\n",
    "        for value in values:\n",
    "            yield key, value\n",
    "\n",
    "    # User-defined Reduce function. Default outputs nothing.\n",
    "    def reduce(self, key, values):\n",
    "        pass\n",
    "\n",
    "    # -- Methods to run the MapReduce program (or parts of it) --------------------------------------------\n",
    "\n",
    "    # Run this MapReduce program on the provided data.\n",
    "    def run(self, data):\n",
    "        return (\n",
    "            self.run_map_combine(data).groupByKey().flatMap(lambda kv: self.reduce(*kv))\n",
    "        )\n",
    "\n",
    "    # Run only the Map part of this program.\n",
    "    def run_map(self, data):\n",
    "        return data.flatMap(lambda kv: self.map(*kv))\n",
    "\n",
    "    # Run only the Map and Combine part of this program.\n",
    "    def run_map_combine(self, data):\n",
    "        return (\n",
    "            self.run_map(data)\n",
    "            .mapPartitions(self.combine_partitions)\n",
    "            .flatMap(lambda kv: self.combine(*kv))\n",
    "        )\n",
    "\n",
    "    # Run only the Map and Reduce part of this program.\n",
    "    def run_map_reduce(self, data):\n",
    "        return self.run_map(data).groupByKey().flatMap(lambda kv: self.reduce(*kv))\n",
    "\n",
    "    # -- internals ----------------------------------------------------------------------------------------\n",
    "\n",
    "    def combine_partitions(self, iterator):\n",
    "        # construct input to combiner\n",
    "        aggregator = defaultdict(list)\n",
    "        i = 0\n",
    "        for item in iterator:\n",
    "            key, value = item\n",
    "            aggregator[key].append(value)\n",
    "        return aggregator.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f51896",
   "metadata": {},
   "source": [
    "## Task 1: Map Reduce\n",
    "\n",
    "The goal of this task is to define several short MapReduce programs to become\n",
    "familiar with MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd14523",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Word Count Example\n",
    "\n",
    "**Input:** A list of tuples `(int, str)` where the integer corresponds to a\n",
    "document ID or a line number and the string to a piece of text.\n",
    "\n",
    "**Output:** A list of tuples `(str, int)` where the string corresponds to a\n",
    "word and the integer to the no. of occurrences of that word in all lines or\n",
    "documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6446d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example program. We use the yield statement to output a key-value pair.\n",
    "class WordCount(MapReduceProgram):\n",
    "    def map(self, id, text):\n",
    "        for word in text.split(\" \"):\n",
    "            yield word, 1\n",
    "\n",
    "    def reduce(self, word, values):\n",
    "        yield word, sum(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d3302",
   "metadata": {},
   "source": [
    "#### Test run with example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4f1517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data (of type RDD<Integer,String>). We first create an array and then\n",
    "# copy it to the Spark cluster.\n",
    "exampleData = sc.parallelize(\n",
    "    [\n",
    "        (1, \"data science is all about data which means that data matters most\"),\n",
    "        (2, \"data driven decisions require a lot of data to make sense of science data\"),\n",
    "        (3, \"without data science is just a guess and more data makes data science better\"),\n",
    "        (4, \"data pipelines process raw data into clean data that science can use to learn from data science\"),\n",
    "        (5, \"while science fiction is called science it often contains little real science or data\"),\n",
    "    ]\n",
    ")\n",
    "exampleData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b131ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "program = WordCount()\n",
    "print(\"Map        : \", program.run_map(exampleData).collect(), end=\"\\n\\n\")\n",
    "print(\"Map+Reduce : \", program.run(exampleData).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45610f1",
   "metadata": {},
   "source": [
    "#### Test run with a Shakespeare text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7fbdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load whole document into an RDD<Integer, String>.\n",
    "shakespeare_with_index = (\n",
    "    sc\n",
    "    .textFile(\"shakespeare.txt\")        # RDD<String>\n",
    "    .zipWithIndex()                     # RDD<String, Int>\n",
    "    .map(lambda t: tuple(reversed(t)))  # RDD<Int, String>\n",
    ")\n",
    "\n",
    "shakespeare_with_index.take(10)  # Note: Data is not cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e19881",
   "metadata": {},
   "outputs": [],
   "source": [
    "program = WordCount()\n",
    "print(\"Map        : \", program.run_map(shakespeare_with_index).take(10))\n",
    "print(\"Map+Reduce : \", program.run(shakespeare_with_index).take(10))\n",
    "\n",
    "# Top-10 words\n",
    "print(\"Top-k words:\")\n",
    "top_words = (\n",
    "    program\n",
    "    .run(shakespeare_with_index)\n",
    "    .sortBy(lambda t: t[1], ascending=False)  # Sort according to the tuple's 2nd element, i.e., the count.\n",
    ")\n",
    "\n",
    "top_words.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfffb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect program like so:\n",
    "print(top_words.toDebugString().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b6fa98",
   "metadata": {},
   "source": [
    "### Document Count Exercise\n",
    "\n",
    "**Input:** Given is a list of `(int, str)` tuples where the integer\n",
    "corresponds to a  document ID (or line number) and the string to the document\n",
    "(or line) content.\n",
    "\n",
    "**Output:** A list of `(str, int)` tuples where the string corresponds to a\n",
    "word and the integer to the no. of occurrences of that word in all documents\n",
    "(or lines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0817449",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class DocumentCount(MapReduceProgram):\n",
    "    def map(self, key, value):\n",
    "        # TODO: Your code here.\n",
    "        ...\n",
    "        # BEGIN_SOLUTION\n",
    "        # break document into list of trimmed words\n",
    "        words = value.split(\" \")\n",
    "        unique_words = set(words)\n",
    "        for unique_word in unique_words:\n",
    "            yield unique_word, 1\n",
    "        # END_SOLUTION\n",
    "\n",
    "    def reduce(self, term, values):\n",
    "        # TODO: Your code here.\n",
    "        ...\n",
    "        # BEGIN_SOLUTION\n",
    "        total = sum(values)\n",
    "        yield term, total\n",
    "        # END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55841f23",
   "metadata": {},
   "source": [
    "#### Test run with example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run and output 10 terms\n",
    "DocumentCount().run(exampleData).collect()\n",
    "\n",
    "# Should give:\n",
    "# [('most', 1),\n",
    "#  ('require', 1),\n",
    "#  ('of', 1),\n",
    "#  ('decisions', 1),\n",
    "#  ('more', 1),\n",
    "# ('and', 1),\n",
    "# ('pipelines', 1),\n",
    "# ('all', 1),\n",
    "# ('that', 2),\n",
    "#  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de951cf4",
   "metadata": {},
   "source": [
    "#### Test run with a shakespeare text (from above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef1f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run and output 10 terms\n",
    "DocumentCount().run(shakespeare_with_index).take(10)\n",
    "\n",
    "# Should give:\n",
    "# [('KING', 1598),\n",
    "#  ('', 49942),\n",
    "#  ('\\tDRAMATIS', 37),\n",
    "#  ('HENRY\\tthe', 4),\n",
    "#  ('HENRY,', 14),\n",
    "#  ('of', 15527),\n",
    "#  ('Wales\\t(PRINCE', 1),\n",
    "#  ('\\t\\t|', 41),\n",
    "#  ('JOHN', 102),\n",
    "#  ('WALTER', 21)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8638af8",
   "metadata": {},
   "source": [
    "### N-Gram Count Exercise\n",
    "\n",
    "An n-gram is a contiguous sequence of `n` words from a given sequence of text,\n",
    "where `n > 0` is a parameter. Your implementation should meet the following\n",
    "requirements:\n",
    "\n",
    "- Before processing, remove all special characters at the beginning and end of\n",
    "  a word. Special characters are all characters other than letters and digits.\n",
    "  To do this, you can use the already existing `trim()` method.\n",
    "- Count all occurrences of n-grams of length n that occur at least `sigma`\n",
    "  times, where `sigma > 0` is another parameter. Both `n` and `sigma` can be\n",
    "  specified via global variables\n",
    "- The output should be pairs of (n-gram, number of occurrences).\n",
    "\n",
    "**Input:** A list of `(int, str)` tuples where the integer corresponds to a\n",
    "line number (or document ID) and the string to a line of text or a document.\n",
    "\n",
    "**Output:** A list of `(str, int)` tuples where the string corresponds to an\n",
    "n-gram and the integer to the no. of occurrences of that n-gram. As described\n",
    "above, the no. of occurrences should be at least `sigma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b0e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters as global variables (use these in your solution)\n",
    "n = 2  # length of n-grams\n",
    "sigma = 2  # minimum frequency to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eac6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trims a single word.\n",
    "def trim(s):\n",
    "    return re.search(r\"^[^\\w\\d]*(.*?)[^\\w\\d]*$\", s).group(1)\n",
    "\n",
    "\n",
    "# example\n",
    "trim(\"!test12,#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0199c4cb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define and implement class NGramCount.\n",
    "class NGramCount(MapReduceProgram):\n",
    "    def map(self, key, value):\n",
    "        # TODO: Your code here.\n",
    "        ...\n",
    "        # BEGIN_SOLUTION\n",
    "        # break document into list of trimmed words\n",
    "        words = [w for w in [trim(w) for w in value.split(\" \")] if len(w) > 0]\n",
    "\n",
    "        # Iterate over all possible ngrams\n",
    "        for i in range(len(words) - n + 1):\n",
    "            j = i + n - 1\n",
    "            ngram = \" \".join(words[i : j + 1])  # ngram starting at i\n",
    "            yield ngram, 1\n",
    "        # END_SOLUTION\n",
    "\n",
    "    def reduce(self, ngram, values):\n",
    "        # TODO: Your code here.\n",
    "        ...\n",
    "        # BEGIN_SOLUTION\n",
    "        total = sum(values)\n",
    "        if total >= sigma:\n",
    "            yield ngram, total\n",
    "        # END_SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b54ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NGramCount().run(exampleData).take(10)\n",
    "\n",
    "# Should give:\n",
    "# [('data science', 4), ('science is', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eefa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run and output 10 ngrams from the Shakespeare text used before.\n",
    "NGramCount().run(shakespeare_with_index).take(10)\n",
    "\n",
    "# Should give:\n",
    "# [('1 KING', 48),\n",
    "#  ('DRAMATIS PERSONAE', 37),\n",
    "#  ('HENRY\\tthe Fourth', 2),\n",
    "#  ('Fourth KING', 2),\n",
    "#  ('Prince of', 29),\n",
    "#  ('SIR WALTER', 19),\n",
    "#  ('EDMUND MORTIMER\\tEarl', 2),\n",
    "#  ('MORTIMER\\tEarl of', 2),\n",
    "#  ('March MORTIMER', 2),\n",
    "#  ('of York', 111)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c776e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result can be stored on disk using `saveAsTextFile`.\n",
    "# ngram_result.saveAsTextFile(f\"shakespeare-{n}grams-{sigma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662c77a8",
   "metadata": {},
   "source": [
    "\n",
    "## Task 2: Apache Spark instead of MapReduce\n",
    "\n",
    "In this task, implement the previous MapReduce programs using Apache Spark.\n",
    "\n",
    "For each task, we will provide a list of transformations and actions that\n",
    "you can use to solve the given problem. Of course, feel free to use your\n",
    "own approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24d827a",
   "metadata": {},
   "source": [
    "### Word Count (Spark)\n",
    "\n",
    "**Input:** (see above)\n",
    "\n",
    "**Output:** (see above)\n",
    "\n",
    "**Possible Transformations & Actions:**\n",
    "\n",
    "- `flatMap`: transform each line into a list of words (split at spaces between words), then flatten\n",
    "- `map`: trim each word using the provided `trim` function\n",
    "- `map`: transform each word into a tuple `(word, 1)`\n",
    "- `reduceByKey`: sum all ones together that belong to a single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe9bd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_result = (\n",
    "     sc\n",
    "    .textFile(\"shakespeare.txt\")\n",
    "    # TODO: Your code here.\n",
    "    # BEGIN_SOLUTION\n",
    "    .flatMap(lambda l: l.split(\" \"))\n",
    "    .map(trim)\n",
    "    .map(lambda w: (w, 1))\n",
    "    .reduceByKey(lambda k, l: k + l)\n",
    "    # END_SOLUTION\n",
    ")\n",
    "\n",
    "word_count_result.take(10)\n",
    "\n",
    "# Should give:\n",
    "\n",
    "# [('KING', 1774),\n",
    "#  ('', 64849),\n",
    "#  ('DRAMATIS', 37),\n",
    "#  ('HENRY\\tthe', 4),\n",
    "#  ('Fourth', 45),\n",
    "#  ('of', 16423),\n",
    "#  ('Wales\\t(PRINCE', 1),\n",
    "#  ('JOHN', 121),\n",
    "#  ('WALTER', 21),\n",
    "#  ('BLUNT', 12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018fa579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting is really easy in Spark (but would require another program in MapReduce).\n",
    "word_count_result_sorted = (\n",
    "    word_count_result\n",
    "    .sortBy(lambda t: t[1], ascending=False)\n",
    ")\n",
    "\n",
    "word_count_result_sorted.take(10)\n",
    "\n",
    "# Should give:\n",
    "# [('', 64849),\n",
    "#  ('the', 25542),\n",
    "#  ('and', 19603),\n",
    "#  ('I', 18419),\n",
    "#  ('to', 16850),\n",
    "#  ('of', 16423),\n",
    "#  ('a', 13254),\n",
    "#  ('you', 12507),\n",
    "#  ('my', 11285),\n",
    "#  ('in', 10545)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b164747",
   "metadata": {},
   "source": [
    "### Document Count (Spark)\n",
    "\n",
    "**Input:** (see above)\n",
    "\n",
    "**Output:** (see above)\n",
    "\n",
    "**Possible Transformations & Actions:**\n",
    "\n",
    "- `flatMap`: Split a document (or line) into words, then trim the word. Keep\n",
    "  the index in the resulting tuples (in the second position).\n",
    "- `groupByKey`: Group all tuples by their keys so that the values\n",
    "  consist of the indices.\n",
    "- `mapValues`: Convert the values into a set (using the `set` constructor).\n",
    "- `mapValues`: Compute the no. of items per set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524fa7c0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "document_count_result = (\n",
    "    sc\n",
    "    .textFile(\"shakespeare.txt\")\n",
    "    .zipWithIndex()\n",
    "    # TODO: Your code here.\n",
    "    # BEGIN_SOLUTION\n",
    "    .flatMap(lambda t: [(trim(word), t[1]) for word in t[0].split(\" \")])\n",
    "    .groupByKey()\n",
    "    .mapValues(set)\n",
    "    .mapValues(lambda v: len(v))\n",
    "    # END_SOLUTION\n",
    ")\n",
    "\n",
    "document_count_result.take(10)\n",
    "\n",
    "# Should give:\n",
    "\n",
    "# [('KING', 1766),\n",
    "#  ('', 50119),\n",
    "#  ('DRAMATIS', 37),\n",
    "#  ('HENRY\\tthe', 4),\n",
    "#  ('Fourth', 45),\n",
    "#  ('of', 15842),\n",
    "#  ('Wales\\t(PRINCE', 1),\n",
    "#  ('JOHN', 121),\n",
    "#  ('WALTER', 21),\n",
    "#  ('BLUNT', 12)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6071dd7c",
   "metadata": {},
   "source": [
    "### n-gram Count (Spark)\n",
    "\n",
    "**Input:** (see above)\n",
    "\n",
    "**Output:** (see above)\n",
    "\n",
    "**Possible Transformations & Actions:**\n",
    "\n",
    "- `flatMap`: Create ngrams using the `create_ngrams` helper function.\n",
    "- `map`: Convert the ngrams into `(ngram, 1)` tuples.\n",
    "- `reduceByKey`: Group by key and add the values together to produce an ngram count.\n",
    "- `filter`: Filter ngrams with less than sigma occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eb30de",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "sigma = 20\n",
    "\n",
    "\n",
    "# Define helper function.\n",
    "def create_ngrams(line):\n",
    "    \"\"\"Given a line of text, produce ngrams of length n.\"\"\"\n",
    "    # TODO: Your code here.\n",
    "    # BEGIN_SOLUTION\n",
    "    words = [w for w in [trim(w) for w in line.split(\" \")] if len(w) > 0]\n",
    "\n",
    "    # Iterate over all possible ngrams\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        j = i + n - 1\n",
    "        ngram = \" \".join(words[i : j + 1])  # ngram starting at i\n",
    "        ngrams.append(ngram)\n",
    "\n",
    "    return ngrams\n",
    "    # END_SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8629c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_result = (\n",
    "    sc\n",
    "    .textFile(\"shakespeare.txt\")\n",
    "    .flatMap(create_ngrams)\n",
    "    # TODO: Your code here.\n",
    "    # BEGIN_SOLUTION\n",
    "    .map(lambda ngram: (ngram, 1))\n",
    "    .reduceByKey(lambda u, v: u + v)\n",
    "    .filter(lambda t: t[1] > sigma)\n",
    "    # END_SOLUTION\n",
    ")\n",
    "\n",
    "ngram_result.take(10)\n",
    "\n",
    "# Should give:\n",
    "\n",
    "# [('1 KING HENRY', 48),\n",
    "#  ('KING HENRY IV', 52),\n",
    "#  ('to tell you', 21),\n",
    "#  ('It is a', 28),\n",
    "#  ('is to be', 34),\n",
    "#  ('so much as', 24),\n",
    "#  ('by and by', 41),\n",
    "#  ('that thou art', 24),\n",
    "#  ('as it is', 30),\n",
    "#  ('I can tell', 38)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12405b",
   "metadata": {},
   "source": [
    "## Task 3: A more complex spark task\n",
    "\n",
    "This task contains a Spark task that is a bit more complicated than the\n",
    "previous ones.\n",
    "\n",
    "**Input:** A social network graph, given as a list of edges as tab-separated \n",
    "values: `str \\t str`. Each relationship only occurs once.\n",
    "\n",
    "**Output:** A list of mutual friends of pairs of network members, i.e., a list\n",
    "of tuples `((str, str), set[str])`.\n",
    "\n",
    "### Example\n",
    "\n",
    "**Input:** \n",
    "```\n",
    "Max       Stefanie\n",
    "Max       Carla\n",
    "Stefanie  Carla\n",
    "Max       Bernhard\n",
    "Stefanie  Bernhard\n",
    "\n",
    "```\n",
    "**Output:**\n",
    "```\n",
    "[\n",
    " (('Bernhard', 'Carla'), {'Max', 'Stefanie'}),\n",
    " (('Max', 'Stefanie'), {'Bernhard', 'Carla'}),\n",
    " (('Bernhard', 'Max'), {'Stefanie'}),\n",
    " (('Carla', 'Max'), {'Stefanie'}),\n",
    " (('Carla', 'Stefanie'), {'Max'}),\n",
    " (('Bernhard', 'Stefanie'), {'Max'})\n",
    "]\n",
    "```\n",
    "\n",
    "**Possible Transformations & Actions:**\n",
    "\n",
    "It may help to approach this problem in several steps.\n",
    "\n",
    "- **Step 1:** Compute adjacency sets per person where each set contains all\n",
    "  friends of that person.\n",
    "  - `map`: Split each input line at the tab character `\\t`.\n",
    "  - `flatMap`: For each friends tuple, create a reversed tuple.\n",
    "  - `groupByKey`: Group all friends per user.\n",
    "  - `mapValues`: Apply the `set` constructor to obtain a set of friends per user.\n",
    "- **Step 2:** Create tuples which contain a single mutual friend per pair of users.\n",
    "  - `flatMap`: For each pair of friends `(friendA, friendB)` of a user `user`,\n",
    "      create a tuple `((friendA, friendB), user)`.\n",
    "- **Step 3:** Group preceding results together to solve the problem.\n",
    "  - `groupByKey`: Group pairs of users together so that the values consist of\n",
    "    their mutual friends.\n",
    "  - `mapValues`: Convert the values into a set.\n",
    "  - (optional) `sortBy`: Sort the results based on the no. of mutual friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4581c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_friends = sc.parallelize([\n",
    "    \"Max\\tStefanie\",\n",
    "    \"Max\\tCarla\",\n",
    "    \"Stefanie\\tCarla\",\n",
    "    \"Max\\tBernhard\",\n",
    "    \"Stefanie\\tBernhard\",\n",
    "])\n",
    "\n",
    "adj_sets = (\n",
    "    # sc.textFile(\"social_network_edges.tsv\")  # actual data (see file)\n",
    "    example_friends  # some example data (see above)\n",
    "    # TODO: Your code here.\n",
    "    # BEGIN_SOLUTION\n",
    "    .map(lambda l: l.split(\"\\t\"))\n",
    "    .flatMap(lambda t: [t, tuple(reversed(t))])\n",
    "    .groupByKey()\n",
    "    .mapValues(set)\n",
    "    # END_SOLUTION\n",
    ")\n",
    "\n",
    "adj_sets.take(5)\n",
    "\n",
    "# Should give:\n",
    "# [('Stefanie', {'Bernhard', 'Carla', 'Max'}),\n",
    "#  ('Bernhard', {'Max', 'Stefanie'}),\n",
    "#  ('Max', {'Bernhard', 'Carla', 'Stefanie'}),\n",
    "#  ('Carla', {'Max', 'Stefanie'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_mutual_friend = (\n",
    "    adj_sets\n",
    "    # TODO: Your code here.\n",
    "    # BEGIN_SOLUTION\n",
    "    .flatMap(\n",
    "        lambda x: [\n",
    "            ((min(f1, f2), max(f1, f2)), x[0]) for f1 in x[1] for f2 in x[1] if f1 < f2\n",
    "        ]\n",
    "    )\n",
    "    # END_SOLUTION\n",
    ")\n",
    "\n",
    "single_mutual_friend.take(5)\n",
    "\n",
    "# Should give:\n",
    "# [(('Carla', 'Max'), 'Stefanie'),\n",
    "#  (('Bernhard', 'Carla'), 'Stefanie'),\n",
    "#  (('Bernhard', 'Max'), 'Stefanie'),\n",
    "#  (('Max', 'Stefanie'), 'Bernhard'),\n",
    "#  (('Bernhard', 'Stefanie'), 'Max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45801ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_friends = (\n",
    "    single_mutual_friend\n",
    "    # TODO: Your code here.\n",
    "    # BEGIN_SOLUTION\n",
    "    .groupByKey()\n",
    "    .mapValues(set)\n",
    "    .sortBy(lambda kv: len(kv[1]), ascending=False)\n",
    "    # END_SOLUTION\n",
    ")\n",
    "\n",
    "mutual_friends.take(10)\n",
    "\n",
    "# Should give:\n",
    "\n",
    "# [(('Bernhard', 'Carla'), {'Max', 'Stefanie'}),\n",
    "#  (('Max', 'Stefanie'), {'Bernhard', 'Carla'}),\n",
    "#  (('Bernhard', 'Max'), {'Stefanie'}),\n",
    "#  (('Carla', 'Max'), {'Stefanie'}),\n",
    "#  (('Carla', 'Stefanie'), {'Max'}),\n",
    "#  (('Bernhard', 'Stefanie'), {'Max'})]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b5bf1",
   "metadata": {},
   "source": [
    "### Bonus: Mutual friends of friends\n",
    "\n",
    "Filter the list of mutual friends so that it only contains mutual friends of befriended users.\n",
    "\n",
    "**Example (continue from above):**\n",
    "```\n",
    "[\n",
    " (('Max', 'Stefanie'), {'Bernhard', 'Carla'}),\n",
    " (('Bernhard', 'Max'), {'Stefanie'}),\n",
    " (('Carla', 'Max'), {'Stefanie'}),\n",
    " (('Carla', 'Stefanie'), {'Max'}),\n",
    " (('Bernhard', 'Stefanie'), {'Max'})\n",
    "]\n",
    "```\n",
    "This means that the tuple `(('Bernhard', 'Carla'), {'Max', 'Stefanie'})` is removed\n",
    "because Bernhard and Carla are not friends.\n",
    "\n",
    "**Possible Transformations & Actions:**\n",
    "\n",
    "- **Step 1:** Compute a list of all friend pairs.\n",
    "  - `map`: Split input at tab characters.\n",
    "  - `flatMap`: Per tuple, additionally create the reversed tuple.\n",
    "  - `collect`: Collect data from the Spark cluster to a local Python list.\n",
    "- **Step 2:** Use the list of friend pairs to filter the previous result.\n",
    "  - `filter`: Keep the result if it exists in `friend_pairs`.\n",
    " \n",
    "**Note:** Step 1 would be solved using a [Broadcast variable](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.Broadcast.html) in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a0b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "friend_pairs = set(\n",
    "    # sc.textFile(\"social_network_edges.tsv\")\n",
    "    example_friends\n",
    "    # TODO: Your code here.\n",
    "    # BEGIN_SOLUTION\n",
    "    .map(lambda l: tuple(l.split(\"\\t\")))\n",
    "    .flatMap(lambda l: [l, tuple(reversed(l))])\n",
    "    .collect()\n",
    "    # END_SOLUTION\n",
    ")\n",
    "\n",
    "friend_pairs\n",
    "\n",
    "# Should give:\n",
    "# {('Bernhard', 'Max'),\n",
    "#  ('Bernhard', 'Stefanie'),\n",
    "#  ('Carla', 'Max'),\n",
    "#  ('Carla', 'Stefanie'),\n",
    "#  ('Max', 'Bernhard'),\n",
    "#  ('Max', 'Carla'),\n",
    "#  ('Max', 'Stefanie'),\n",
    "#  ('Stefanie', 'Bernhard'),\n",
    "#  ('Stefanie', 'Carla'),\n",
    "#  ('Stefanie', 'Max')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeecd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_friends_of_friends = (\n",
    "    mutual_friends\n",
    "    # TODO: Your code here.\n",
    "    # BEGIN_SOLUTION\n",
    "    .filter(lambda kv: kv[0] in friend_pairs)\n",
    "    # END_SOLUTION\n",
    ")\n",
    "\n",
    "mutual_friends_of_friends.take(10)\n",
    "\n",
    "# Should give: \n",
    "\n",
    "# [(('Max', 'Stefanie'), {'Bernhard', 'Carla'}),\n",
    "#  (('Bernhard', 'Max'), {'Stefanie'}),\n",
    "#  (('Carla', 'Max'), {'Stefanie'}),\n",
    "#  (('Carla', 'Stefanie'), {'Max'}),\n",
    "#  (('Bernhard', 'Stefanie'), {'Max'})]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9193e3",
   "metadata": {},
   "source": [
    "## Understanding RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b14be8d",
   "metadata": {},
   "source": [
    "To use RDDs effectively, it is important to understand the underlying dataflow\n",
    "concept so that (1) your Spark program outputs what it should and (2) your\n",
    "Spark program does not compute more than it should (and than you may expect).\n",
    "\n",
    "The program below will showcase these points. It has a number flaws, which\n",
    "we first identify and then fix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fe7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(range(0, 128000))\n",
    "sample = sc.parallelize(a)\n",
    "while sample.count() > 1000:\n",
    "    sample = sample.filter(lambda x: random.random() > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c93a0a",
   "metadata": {},
   "source": [
    "### a)\n",
    "Suppose we run the program five times (but do not execute the program\n",
    "yet.) Which outputs do you expect on line 5? How often do you think the\n",
    "random number generator is called on average during the second iteration\n",
    "of the while loop?\n",
    "\n",
    "BEGIN_SOLUTION\n",
    "\n",
    "If we think RDDs as a collection of values, we may expect a value ≤ 1000 on line 6. If the random number\n",
    "generator generates numbers with equal probabilities, then on an average it is called 64000 times during\n",
    "the second iteration. We show below that this reasoning is flawed.\n",
    "\n",
    "END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac303e",
   "metadata": {},
   "source": [
    "### b)\n",
    "Execute the program five times in Spark. Which outputs do you get? Were your\n",
    "expectations met?\n",
    "\n",
    "BEGIN_SOLUTION\n",
    "\n",
    "On executing the program 5 times (note that this may be different each time you run): 1045, 1004, 961,\n",
    "1000, 483\n",
    "\n",
    "END_SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4919df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c952456f",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "The program can output a sample size larger than 1000. Why?\n",
    "\n",
    "BEGIN_SOLUTION\n",
    "\n",
    "When the action count is invoked in line 6, the RDD is again computed according to the dataflow\n",
    "rdd.parallelize().filter().filter()...filter(). This can result in a value larger than 1000 because\n",
    "random numbers are regenerated every time and make take different values.\n",
    "\n",
    "END_SOLUTION\n",
    "\n",
    "### d)\n",
    "\n",
    "Assume that there are no failures in the cluster while running your program.\n",
    "Suggest how to fix the above program using the cache. (The goal is to get an\n",
    "RDD representing a sample of size at most 1000.)\n",
    "\n",
    "BEGIN_SOLUTION\n",
    "\n",
    "On an average in the first iteration, random number generator is called 128 000 times. In the second\n",
    "iteration, it is called 128 000 + 64 000 = 192 000 because the data flow is rdd.parallelize().filter()-\n",
    ".filter().\n",
    "\n",
    "END_SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa0918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here.\n",
    "# BEGIN_SOLUTION\n",
    "a = list(range(0, 128000))\n",
    "sample = sc.parallelize(a)\n",
    "while sample.count() > 1000:\n",
    "    sample = sample.filter(lambda _: random.random() > 0.5).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f86f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample.count())\n",
    "# END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a83e72f",
   "metadata": {},
   "source": [
    "### e)\n",
    "\n",
    "In your fix, how much data will be cached by Spark? (If in answering this\n",
    "question you spot a problem in your fix, fix it.)\n",
    "\n",
    "BEGIN_SOLUTION\n",
    "\n",
    "Since the above solution caches RDDs after each iteration, it caches on an average 64000+32000+16000+\n",
    "· · · + 2000 + 1000 values. Below is a fix:\n",
    "\n",
    "END_SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb2c5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here.\n",
    "# BEGIN_SOLUTION\n",
    "a = list(range(0, 128000))\n",
    "sample = sc.parallelize(a)\n",
    "while sample.count() > 1000:\n",
    "    old_sample = sample\n",
    "    sample = sample.filter(lambda _: random.random() > 0.5).cache()\n",
    "    old_sample.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbbeaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f21c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.unpersist()\n",
    "# END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8075215",
   "metadata": {},
   "source": [
    "### f)\n",
    "\n",
    "Assume that there can be node failures. Is your fix still correct? If yes,\n",
    "why? If not, why not? Modify your fix so that it works in case of up to one\n",
    "node failure.\n",
    "\n",
    "Hint: have a look at the [Storage Level\n",
    "documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c1459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here.\n",
    "# BEGIN_SOLUTION\n",
    "a = list(range(0, 128000))\n",
    "sample = sc.parallelize(a)\n",
    "while sample.count() > 1000:\n",
    "    old_sample = sample\n",
    "    sample = sample.filter(lambda _: random.random() > 0.5).persist(\n",
    "        pyspark.StorageLevel.MEMORY_ONLY_2\n",
    "    )\n",
    "    old_sample.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1ed95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad6154",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.unpersist()\n",
    "# END_SOLUTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
